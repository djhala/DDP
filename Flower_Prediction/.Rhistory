mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
z <- x-1.077
z<-z*w
z<-z/w
z<-z^2
z<-z*w
sum(z)
z<-x-.1471
z<-z^2
z<-z*w
sum(z)
z<-x-mean(x)
z<-z^2
z<-z*w
sum(z)
z<-x-.3
z<-z^2
z<-z*w
sum(z)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
lm(y~x)
r<-lm(y~x)
summary(r)
data(mtcars)
)
x <- mtcars$wt
y <- mtcars$mpg
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
?mtcars
newdata <- data.frame(wt=3.00)
predict(fit, newdata, interval = ("confidence"))
data(mtcars)
fit <- lm(mpg ~ wt, data = mtcars)
newdata <- data.frame(wt=3.00)
predict(fit, newdata, interval = ("confidence"))
predict(fit, newdata, interval = ("predict"))
?predict
?interval
x <- mtcars$wt
y <- mtcars$mpg
x<-x/2
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
x <- mtcars$wt
y <- mtcars$mpg
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
x<-x/2
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
x <- mtcars$wt
x<-x/2
M<-mean(x)
fit <- lm(y ~ x)
summary(fit)
predict(fit , newdata = data.frame(x=M), interval="confidence")
x <- mtcars$wt
y <- mtcars$mpg
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
x<-x/2
M <- mean(x)
fit <- lm(y ~ x)
predict(fit , newdata = data.frame(x=M), interval="confidence")
predict(fit , newdata = data.frame(M), interval="confidence")
predict(fit , newdata = data.frame(x=M), interval="confidence")
summary(fit)
?predict.lm
predict(fit , interval="prediction")
predict(fit , newdata = data.frame(x=M), interval="prediction")
predict(fit , newdata = data.frame(x=M), interval="none")
predict(fit , newdata = data.frame(x=M), interval="confidence", type="terms")
predict(fit , newdata = data.frame(x=M), interval="confidence", type="response")
fit3<-fit
sumCoef<-summary(fit3)$coefficients
sumCoef[2,1]+c(-1,1)*qt(.975,df=fit3$df)*sumCoef[2,2]
x <- mtcars$wt
y <- mtcars$mpg
M <- mean(x)
fit <- lm(y ~ x)
anova(fit)
p<-anova(fit)
summary(p)
summary(fit)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
install.packages("Caret")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
train = createDataPartition(diagnosis, p = 0.50,list=FALSE)
test = createDataPartition(diagnosis, p = 0.50,list=FALSE)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
summary(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
?cut2
cut2()
library(hmisc)
library(Hmisc)
install.packages("Hmisc")
?cut2
library(Hmisc)
?cut2
str(training)
cut2(training)
cut2(training$FlyAsh)
cut2(training$Age)
str(training)
y<- training$CompressiveStrength
age<- training$Age
plot (age, y, pch = 19, col = cut2(age))
plot (y, age, pch = 19, col = cut2(age))
plot (age, y, pch = 19, col = cut2(age))
ash<-training$FlyAsh
plot (ash, y, pch = 19, col = cut2(ash))
cem<- training$Cement
plot (cem, y, pch = 19, col = cut2(cem))
plot (age, y, pch = 19, col = cut2(age))
plot (ash, y, pch = 19, col = cut2(ash))
bla<-training$BlastFurnaceslag
plot (bla, y, pch = 19, col = cut2(bla))
bla<-training[[2]]
plot (bla, y, pch = 19, col = cut2(bla))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
summary(training$Superplasticizer)
summary(log(training$Superplasticizer))
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?PreProcess
?preProcess
colnames(training)
preProcess.default(x = training[, c(58:69)], method = "pca", thresh = 0.9)
preProcess(x = training[, c(58:69)], method = "pca", thresh = 0.9)
preProcess(x = training[, c(58:69)], method = "pca", thresh = 0.8)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ntraining<-training[,c(58:69)]
?train
train(ntraining,training[,131], method="glm")
install.packagse(e1071)
install.packages("e1071")
train(ntraining,training[,131], method="glm")
library(caret)
detach("package:caret", unload=TRUE)
library("caret", lib.loc="C:/Users/Brett Condron/Documents/R/win-library/3.1")
train(ntraining,training[,131], method="glm")
detach("package:Formula", unload=TRUE)
detach("package:ggplot2", unload=TRUE)
detach("package:graphics", unload=TRUE)
detach("package:grDevices", unload=TRUE)
detach("package:grid", unload=TRUE)
detach("package:Hmisc", unload=TRUE)
detach("package:lattice", unload=TRUE)
detach("package:methods", unload=TRUE)
detach("package:stats", unload=TRUE)
detach("package:survival", unload=TRUE)
detach("package:splines", unload=TRUE)
detach("package:utils", unload=TRUE)
detach("package:caret", unload=TRUE)
library("caret", lib.loc="C:/Users/Brett Condron/Documents/R/win-library/3.1")
train(ntraining,training[,131], method="glm")
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ntraining<-training[, c(58:69)]
preProc <- preProcess(training[,58:69], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[,58:69])
modelFit <- train(training$diagnosis ~., method="glm",data=trainPC)
modelFit
?train
preProc <- preProcess(training[,58:69], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[,58:69])
modelFit <- train(training$diagnosis ~ ntraining, method="glm")
class(ntraining)
train(ntraining, training$diagnosis, method="glm")
train(trainPC, training$diagnosis, method="glm")
try<-train(ntraining, training$diagnosis, method="glm")
try(testing)
summary(try,testing)
tried<-train(trainPC, training$diagnosis, method="glm")
tried(testing)
tried
tried(testing)
preProc <- preProcess(training[,58:69], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[,58:69])
modelFit <- train(training$diagnosis ~., method="glm",data=trainPC)
modelFit(testing)
modelFit
try(testing)
tried(testing)
library(caret)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
colnames(training)
preProc <- preProcess(training[,58:69], method="pca", thresh=0.8)
trainPC <- predict(preProc, training[,58:69])
modelFit <- train(training$diagnosis ~., method="glm",data=trainPC)
modelfit
modelFit
que<-predict(modelFit, testing)
modelFit <- train(diagnosis ~[,58:69], method="glm",data=training)
modelFit <- train(diagnosis ~ "^IL", method="glm",data=training)
modelFit <- train(diagnosis ~ ^"IL", method="glm",data=training)
modelFit <- train(diagnosis ~ "^IL", method="glm",data=training)
modelFit <- train(diagnosis ~ training[,58:69], method="glm",data=training)
modelFit <- train(diagnosis ~ training[[,58:69]], method="glm",data=training)
ntrain<-training["^IL"]
ntrain<-training[,"^IL"]
ntrain<-training[,58:69]
ntrain<-cbind(ntrain,training$diagnosis)
modelFit <- train(diagnosis ~ ., method="glm",data=ntrain)
set.seed(3433)
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
ntrain<-training[,58:69]
ntrain<-cbind(ntrain,training$diagnosis)
modelFit <- train(diagnosis ~., method="glm",data=ntrain)
ntrain
colnames(ntrain)
class(ntrain$IL_11)
modelFit <- train(diagnosis ~., method="glm",data=ntrain)
summary(ntrain)
modelFit <- train(diagnosis ~., method="glm",data=ntrain)
lm(diagnosis~., data=ntrain)
?model.frame.default
lm(diagnosis~IL_11, data=ntrain)
lm(diagnosis~'IL_11', data=ntrain)
lm(diagnosis~IL_11, data=ntrain)
lm(diagnosis~IL_11, data=training)
?subset
ntrain<-subset(training, select=58:69)
colnames(ntrain)
ntrain<-cbind(ntrain,training$diagnosis)
lm(diagnosis~IL_11, data=training)
lm(diagnosis~IL_11, data=ntrain)
?preProcess
preProc <- preProcess(training[,58:69], method="pca", thresh=0.8)
preProc<-cbind(Preproc,training$diagnosis)
preProc<-cbind(preProc,training$diagnosis)
lm(diagnosis, 58:69, data=training)
colnames(training)
mod<-colnames(training[,58:69])
mod
lm(diagnosis ~ mod, data=training)
mod[1]
mod[[1]]
mod[[]]
mod[]
lm(diagnosis ~ mod[], data=training)
lm(diagnosis ~ for (x in mod){
paste(x,sep="+")
}, data=training)
g<-for (x in mod){
paste(x,sep="+")
}
g
?paste
g<-for (x in mod){
k<-paste(k,x,sep="+")
}
k<-""
g<-for (x in mod){
k<-paste(k,x,sep="+")
}
g
for (x in mod) print x
for (x in mod) print (x)
for (x in mod) paste (x)
g<-for (x in mod) paste (x)
g
modelFit <- train(diagnosis ~., method="glm",data=training)
modelFit
modelFit <- train(diagnosis ~ subset(training, select=58:69), method="glm",data=training)
mod
colnames(training)
ntrain<-training[,c(1,58:69)]
lm(diagnosis~., data=ntrain)
modelFit <- train(diagnosis ~., method="glm",data=training)
modelFit <- train(diagnosis ~., method="glm",data=ntrain)
modelFit
predict(modelFit,testing)
mean(testing$diagnosis == predict(modelFit, testing))
preProc <- preProcess(training[,58:69], method="pca", thresh=0.9)
preProc
install.packages("slidify")
install.packages("rattle")
install.packages("rattle")
---
output: pdf_document
---
## Executive Summary
This report analyzes data from the 1974 Motor Trend US magazine to try to determine if manual or automatic cars get better gas mileage. Further, it attempts to quantify the difference.
It is determined that the data presented does not indicate that transmission type has a meaningful impact on fuel economy.
## Phase 1: Literature Review
Prior to engaging in exploratory data analysis, some minor literature review on features affecting fuel economy was performed. According to the 1976 EPA document "Factors Affecting Automotive Fuel Economy", the two single most important features are weight and displacement. Accordingly, our baseline linear model will contain only the features that the literature deemed important.
Efforts to access additional literature on the subject were hampered by the Great Firewall.
## Phase 2: Exploratory Analysis
Graphs and data tables are located in the appendix. First a summary of the data was reviewed to ensure there were no problems with missing data. The data was determined to be usable.
Then graphs were created of mileage against all other features in the data. The expected relationship of mileage to displacement and weight was observed. Cars with manual transmission did appear to have better fuel economy on average than cars with automatic transmissions.
Graphs were created of mileage against all other features except transmission type. These graphs were colored by transmission type. It was immediately apparent that the cars that were heaviest had automatic transmissions, and the cars that were lightest had manual transmissions. Unsurprisingly, these were also at the two extremes of fuel efficiency. The same pattern was observed with displacement. The cars with the greatest displacement, and thus lowest fuel economy, were all automatic.
In the case of both displacement and weight, when only cars with similar displacement and weight were looked at, manual cars performed about the same as or worse than equivalent manual cars, as can be seen in the following plot where blue represents manual and green automatic.
```{r, echo=FALSE}
data(mtcars)
midweightcars<-subset(mtcars, wt>=2.3)
midweightcars<-subset(midweightcars, wt<=4)
middispcars<-subset(mtcars, disp<=370)
middispcars<-subset(middispcars, disp>=230)
par(mfrow=c(1,2))
plot(mpg~wt, col=am+3, data=midweightcars)
plot(mpg~disp, col=am+3, data=middispcars)
```
With this data in mind, the correlation table for the entire data set was reviewed. It should be noted that many features in the data set were highly correlated. Accordingly they were excluded from all except the trivial 'fitall' model.
There were three features less correlated than transmission type with mileage. Those were the number of gears, the number of carburetors, and the 1/4 mile time. Other features such as rear axle ratio and whether car had a V or single line engine were more correlated with MPG than transmission.
The tables and charts from the exploratory section are at the end of this report, excepting the one immediately above.
## Phase 3: Searching For a Model
A variety of linear models were attempted. The first was a baseline model from our literature review, which we hope to improve upon. The second was a trivial model with all features from the data. The third was the baseline model plus transmission type. The residual graphs below indicate that adding transmission type to the model did not impact the residuals in a meaningful way.
```{r, echo=FALSE}
fitbl<-lm(mpg~wt+disp, data=mtcars)
fitall<-lm(mpg~., data=mtcars)
fitblt<-lm(mpg~wt+disp+am, data=mtcars)
par(mfrow=c(1,3))
plot(density(resid(fitbl)))
plot(density(resid(fitall)))
plot(density(resid(fitblt)))
```
A full summary of the models follows. It can be observed that the blt (baseline plus transmission) model indicates that a manual should get, Cetus Paribus, an extra .17 miles to the gallon. The finding is, however, not significant. The P value and adjusted r-squared value for the model are worse than for the baseline model. The Pr(>|t|) for the transmission variable was .9, or highly insignificant.
In the fitall model, the prediction is an extra 2.5 miles per gallon for a manual transmission, but the Pr(>|t|) is .234, which is still outside the desired range. Moreover, the p value for the fitall model is the worst of the three.
```{r}
summary(fitbl)
summary(fitall)
summary(fitblt)
```
Ultimately, the baseline model provided the best p value and the second best adjusted r-squared value. Accordingly, there is no compelling reason to abandon the baseline model indicated by the literature. It is thus determined that there is insufficient evidence to conclude that transmission type has an impact on fuel economy.
## Further Reasearch
Future research on the subject matter may include using more sophisticated models, which were forbidden by this assignment. Ideally, data from this millennium with information about cars of the same make/model where the only difference is transmission type would be used or obtained in order to make a truly useful analysis.
## Apendix
### Summary of Data
```{r, echo=FALSE}
##summary(mtcars)
k<-as.data.frame(cor(mtcars))
```
### Correlation Table
```{r, echo=FALSE}
k
```
### Exploratory Graphs
```{r, echo=FALSE}
par(mfrow=c(2,5))
plot(mpg~., data=mtcars)
plot(mpg~cyl, col=am+3, data=mtcars)
plot(mpg~disp, col=am+3, data=mtcars)
plot(mpg~hp, col=am+3, data=mtcars)
plot(mpg~drat, col=am+3, data=mtcars)
plot(mpg~wt, col=am+3, data=mtcars)
plot(mpg~qsec, col=am+3, data=mtcars)
plot(mpg~vs, col=am+3, data=mtcars)
plot(mpg~gear, col=am+3, data=mtcars)
plot(mpg~carb, col=am+3, data=mtcars)
```
summary(fitall)
fitall
fitall$coef
colnames(fitall)
names(fitall)
fitall$model
fitall$residuals
names(fitbl)
fitbl$effects
fitbl$rank
summary(fitbl)
names9fitbl
names(fitbl)
fitbl$qr
round
?round
round(k, digits 4)
k
round(k, digits =4)
round(k, digits =3)
library(slidify)
install.packages("slidify")
install.packages("slidify")
install.packages("Slidify")
library(devtools)
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
setwd("C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod")
author('Flower_Prediction')
knit        : slidify::knit2slides
library(knitr)
---
data(iris)
summary(iris)
tables(iris$Species)
table(iris$Species)
library(caret)
library(randomForest)
load('/App-1/fit.Rdata')
setwd("C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1")
load('fit.Rdata')
load("fit2.Rdata")
load('/Class/shiny/devdatprod/App-1/fit1.Rdata')
load('C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit1.Rdata')
load('/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit1.Rdata')
setwd("C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1")
load("C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit.Rdata")
?require
load('C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit.Rdata')
load('C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit.Rdata')
load('C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/App-1/fit2.Rdata')
library(caret)
library(lattice)
library(ggplot2)
library(randomForest)
library(rpart)
#library(rpart.plot)
#fancyRpartPlot(fit1$finalModel)
k<-data.frame(Sepal.Length=4.3, Sepal.Width=2,
Petal.Length=1, Petal.Width=2)
p<-predict(fit1,k)
p[1]
p[[1]]
p
as.character(p)
print(paste("Our main tree thinks you have a ",as.character(predict(fit1,k))))
print(paste("Our second opinion algorithm thinks you have a",as.character(predict(fit2,k)))
)
setwd("C:/Users/Brett Condron/Desktop/Workspace/Class/shiny/devdatprod/Flower_Prediction")
publish()
?publish
publish('bcondron','devdatprod')
publish(user='bcondron', repo='devdatprod')
publish(user='bcondron', repo='devdatprod')
publish(user='bcondron', repo='devdatprod')
publish(user='bcondron', repo='devdatprod')
